{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,Activation,Dropout,BatchNormalization\n",
    "from keras.constraints import maxnorm\n",
    "from keras.optimizers import SGD, Adam\n",
    "import keras.utils\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "import datetime\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_validation_np():\n",
    "    dataset = np.load('../data/npy/validation.npy',mmap_mode='r')\n",
    "    X = dataset[:,0:92]\n",
    "    Y = dataset[:,92:] # :96\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGenerator(keras.utils.Sequence):\n",
    "    'Generates data for Keras'\n",
    "    def __init__(self, batch_size=1024, shuffle=True):\n",
    "        'Initialization'\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.data = np.load('../data/npy/train.npy',mmap_mode='r')\n",
    "        # self.data = np.memmap('full_dataset/train.buffer', dtype=np.float16, mode='r',shape=(255519715, 99))\n",
    "        # rows = int(self.data.shape[0] / 99)\n",
    "        # self.data = np.reshape(self.data,(rows,99))\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        'Denotes the number of batches per epoch'\n",
    "        return int(np.floor(self.data.shape[0] / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generate one batch of data'\n",
    "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
    "        X, y = self.__data_generation(indexes)\n",
    "        return X, y\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        'Updates indexes after each epoch'\n",
    "        self.indexes = np.arange(self.data.shape[0])\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.indexes)\n",
    "\n",
    "    def __data_generation(self, list_IDs_temp):\n",
    "        'Generates data containing batch_size samples'\n",
    "        X = np.empty((self.batch_size, 92),dtype=np.float16)\n",
    "        y = np.empty((self.batch_size, 7),dtype=np.float16)\n",
    "        for i, ID in enumerate(list_IDs_temp):\n",
    "            X[i,] = self.data[ID,:92]\n",
    "            y[i] = self.data[ID,92:]\n",
    "        return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def split_activation(l):\n",
    "    l_s = tf.keras.activations.sigmoid(l[...,0:3])\n",
    "    l_t = tf.keras.activations.tanh(l[...,3:])\n",
    "    lnew = tf.concat([l_s, l_t], axis = 1)\n",
    "    return lnew\n",
    "\n",
    "def get_model():\n",
    "    model = Sequential()\n",
    "    \n",
    "    hidden = 2\n",
    "    widest_layer = 256\n",
    "    output_width = 7\n",
    "    \n",
    "    # model.add(Dense(92, input_shape = (92,)))\n",
    "    # model.add(Dropout(0.1))\n",
    "    model.add(BatchNormalization(input_shape = (92,)))\n",
    "    for i in range(hidden):\n",
    "        size = widest_layer# - i*widest_layer/hidden\n",
    "        # model.add(Dense(size,kernel_constraint=maxnorm()))\n",
    "        model.add(Dense(size,use_bias=False))#,kernel_constraint=maxnorm()))\n",
    "        model.add(BatchNormalization())\n",
    "        # model.add(Activation(\"relu\"))\n",
    "        model.add(tf.keras.layers.LeakyReLU())\n",
    "    \n",
    "    model.add(Dense(output_width,activation=split_activation))#,kernel_constraint=maxnorm()))\n",
    "    sgd = SGD(lr=0.01, momentum=0.99, nesterov=False)\n",
    "    weights = calculating_class_weights()\n",
    "    # weights = np.array([[0.59429336, 3.15130026], [ 0.51234971, 20.74339526], [0.51744448, 14.83118107 ]])\n",
    "    model.compile(loss=get_weighted_loss(weights), optimizer='adam', metrics=[])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculating_class_weights():\n",
    "    y = np.load('../data/npy/train.npy',mmap_mode='r')\n",
    "    # y = np.memmap('full_dataset/train.buffer', dtype=np.float16, mode='r',shape=(255519715, 99))\n",
    "    # rows = int(y.shape[0] / 99)\n",
    "    # y = np.reshape(y,(rows,99))\n",
    "    y = y[:,92:95]\n",
    "    y_labels = np.array(y)\n",
    "    y_labels = np.round(y_labels)\n",
    "    del y\n",
    "    from sklearn.utils.class_weight import compute_class_weight\n",
    "    number_dim = np.shape(y_labels)[1]\n",
    "    weights = np.empty([number_dim, 2])\n",
    "    for i in range(number_dim):\n",
    "        weights[i] = compute_class_weight('balanced', [0.,1.], y_labels[:, i])\n",
    "        print(weights[i])\n",
    "    # weights[1] = weights[1]*1.5 # trying to put more emphasis on correctness of jumps\n",
    "    # weights[3] = weights[3]*1.5 # and dodges\n",
    "    return weights\n",
    "\n",
    "def get_weighted_loss(weights):\n",
    "    def weighted_loss(y_true, y_pred):\n",
    "        # y_true = tf.where(tf.math.is_nan(y_true), y_pred, y_true) # assume correct if nan in true values [meaning that guess does not matter]\n",
    "        y_true_binary = y_true[:,0:3]\n",
    "        y_pred_binary = y_pred[:,0:3]\n",
    "        y_true_analog = y_true[:,3:]\n",
    "        y_pred_analog = y_pred[:,3:]\n",
    "        loss_binary = keras.backend.mean((weights[:,0]**(1-y_true_binary))*(weights[:,1]**(y_true_binary))*keras.backend.binary_crossentropy(y_true_binary, y_pred_binary),axis=0)\n",
    "        loss_analog = keras.backend.abs(keras.backend.sqrt(keras.backend.mean(keras.backend.square((y_true_analog-y_pred_analog)),axis=0)))\n",
    "        # squared_error = keras.backend.square((y_true_analog-y_pred_analog))\n",
    "        # squared_weighted_error = squared_error*[1.0,0.0,1.0,3.0]\n",
    "        # loss_analog = keras.backend.abs(keras.backend.sqrt(keras.backend.mean(squared_weighted_error,axis=-1)))\n",
    "        y = tf.concat((loss_binary,loss_analog),axis=0)\n",
    "        # y = loss_binary*3./7. + loss_analog*4./7.\n",
    "        y = y*[1.0,1.0,1.0,1.0,1.0,1.0,2.0] # steer/yaw importance * 2\n",
    "        # return keras.backend.mean(y)\n",
    "        return y\n",
    "    return weighted_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.59566412 3.11330993]\n",
      "[ 0.51033345 24.69328   ]\n",
      "[ 0.51292848 19.83714653]\n",
      "Epoch 1/10\n",
      "1204/1205 [============================>.] - ETA: 0s - loss: 0.4850\n",
      "Epoch 00001: val_loss improved from inf to 0.53076, saving model to best_01_0.5308.h5\n",
      "\n",
      "Epoch 00001: saving model to latest.h5\n",
      "1205/1205 [==============================] - 8s 6ms/step - loss: 0.4849 - val_loss: 0.5308\n",
      "Epoch 2/10\n",
      "1199/1205 [============================>.] - ETA: 0s - loss: 0.4368\n",
      "Epoch 00002: val_loss did not improve from 0.53076\n",
      "\n",
      "Epoch 00002: saving model to latest.h5\n",
      "1205/1205 [==============================] - 7s 6ms/step - loss: 0.4369 - val_loss: 0.5360\n",
      "Epoch 3/10\n",
      "1198/1205 [============================>.] - ETA: 0s - loss: 0.4183\n",
      "Epoch 00003: val_loss did not improve from 0.53076\n",
      "\n",
      "Epoch 00003: saving model to latest.h5\n",
      "1205/1205 [==============================] - 7s 6ms/step - loss: 0.4183 - val_loss: 0.5598\n",
      "Epoch 4/10\n",
      "1193/1205 [============================>.] - ETA: 0s - loss: 0.4043\n",
      "Epoch 00004: val_loss did not improve from 0.53076\n",
      "\n",
      "Epoch 00004: saving model to latest.h5\n",
      "1205/1205 [==============================] - 7s 6ms/step - loss: 0.4044 - val_loss: 0.5620\n",
      "Epoch 5/10\n",
      "1200/1205 [============================>.] - ETA: 0s - loss: 0.3933\n",
      "Epoch 00005: val_loss did not improve from 0.53076\n",
      "\n",
      "Epoch 00005: saving model to latest.h5\n",
      "1205/1205 [==============================] - 7s 6ms/step - loss: 0.3933 - val_loss: 0.5929\n",
      "Epoch 6/10\n",
      "1193/1205 [============================>.] - ETA: 0s - loss: 0.3830\n",
      "Epoch 00006: val_loss did not improve from 0.53076\n",
      "\n",
      "Epoch 00006: saving model to latest.h5\n",
      "1205/1205 [==============================] - 7s 6ms/step - loss: 0.3830 - val_loss: 0.6074\n",
      "Epoch 7/10\n",
      "1197/1205 [============================>.] - ETA: 0s - loss: 0.3746\n",
      "Epoch 00007: val_loss did not improve from 0.53076\n",
      "\n",
      "Epoch 00007: saving model to latest.h5\n",
      "1205/1205 [==============================] - 9s 7ms/step - loss: 0.3745 - val_loss: 0.6257\n",
      "Epoch 8/10\n",
      "1196/1205 [============================>.] - ETA: 0s - loss: 0.3664\n",
      "Epoch 00008: val_loss did not improve from 0.53076\n",
      "\n",
      "Epoch 00008: saving model to latest.h5\n",
      "1205/1205 [==============================] - 8s 7ms/step - loss: 0.3663 - val_loss: 0.7022\n",
      "Epoch 9/10\n",
      "1194/1205 [============================>.] - ETA: 0s - loss: 0.3606\n",
      "Epoch 00009: val_loss did not improve from 0.53076\n",
      "\n",
      "Epoch 00009: saving model to latest.h5\n",
      "1205/1205 [==============================] - 8s 6ms/step - loss: 0.3607 - val_loss: 0.6854\n",
      "Epoch 10/10\n",
      "1200/1205 [============================>.] - ETA: 0s - loss: 0.3553\n",
      "Epoch 00010: val_loss did not improve from 0.53076\n",
      "\n",
      "Epoch 00010: saving model to latest.h5\n",
      "1205/1205 [==============================] - 8s 7ms/step - loss: 0.3552 - val_loss: 0.6802\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x244b34cc6c8>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = get_model()\n",
    "# weights = np.array([[0.59429336, 3.15130026], [ 0.51234971, 20.74339526], [0.51744448, 14.83118107 ]])\n",
    "# model = keras.models.load_model('../models/latest.h5',custom_objects={'split_activation': split_activation, 'weighted_loss': get_weighted_loss(weights)})\n",
    "# log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "# tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "mc = ModelCheckpoint('../models/best_{epoch:02d}_{val_loss:.4f}.h5', monitor='val_loss', save_best_only=True, verbose=1)\n",
    "mc2 = ModelCheckpoint('../models/latest.h5', monitor='val_loss', save_best_only=False, verbose=1)\n",
    "X_val, Y_val = get_validation_np()\n",
    "g = DataGenerator(batch_size=128, shuffle=True)\n",
    "model.fit(g, validation_data=(X_val,Y_val), epochs=5, callbacks=[mc,mc2], verbose=1, workers=4, max_queue_size=32, initial_epoch=0)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  },
  "metadata": {
   "interpreter": {
    "hash": "23072f189d23521f0e2f0ef3ca1ae80a2b2c9a98e62432ad2740ad39146e1b54"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}